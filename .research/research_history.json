{
  "research_topic": "Learning rate optimization for fine-tuning Qwen3-0.6B on GSM8K elementary math problems",
  "queries": [
    "Qwen3-0.6B fine-tuning",
    "fine-tuning learning rate",
    "GSM8K elementary math",
    "transformer LR schedule",
    "adaptive optimizer tuning"
  ],
  "research_study_list": [
    {
      "title": "QLoRA: Efficient Finetuning of Quantized LLMs"
    },
    {
      "title": "QLoRA: Efficient Finetuning of Quantized LLMs"
    },
    {
      "title": "QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models"
    },
    {
      "title": "QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation"
    },
    {
      "title": "Evaluating Quantized Large Language Models"
    },
    {
      "title": "Rethinking the Hyperparameters for Fine-tuning"
    },
    {
      "title": "Multirate Training of Neural Networks"
    },
    {
      "title": "Multirate Training of Neural Networks"
    },
    {
      "title": "AutoLRS: Automatic Learning-Rate Schedule by Bayesian Optimization on the Fly"
    },
    {
      "title": "How to prepare your task head for finetuning"
    },
    {
      "title": "A Careful Examination of Large Language Model Performance on Grade School Arithmetic"
    },
    {
      "title": "Language models are multilingual chain-of-thought reasoners"
    },
    {
      "title": "OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset"
    },
    {
      "title": "OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset"
    },
    {
      "title": "Evaluating Large Vision-and-Language Models on Children's Mathematical Olympiads"
    },
    {
      "title": "AutoLRS: Automatic Learning-Rate Schedule by Bayesian Optimization on the Fly"
    },
    {
      "title": "On Layer Normalization in the Transformer Architecture"
    },
    {
      "title": "Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations"
    },
    {
      "title": "LEMON: Lossless model expansion"
    },
    {
      "title": "Small-scale proxies for large-scale Transformer training instabilities"
    },
    {
      "title": "MADA: Meta-Adaptive Optimizers Through Hyper-Gradient Descent"
    },
    {
      "title": "Reverse engineering learned optimizers reveals known and novel mechanisms"
    },
    {
      "title": "How Does Adaptive Optimization Impact Local Neural Network Geometry?"
    },
    {
      "title": "Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective"
    },
    {
      "title": "AGD: an Auto-switchable Optimizer using Stepwise Gradient Difference for Preconditioning Matrix"
    }
  ]
}